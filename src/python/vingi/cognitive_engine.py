#!/usr/bin/env python3
"""
Vingi Advanced Cognitive Engine

Implements sophisticated cognitive load optimization through:
- Metacognitive orchestration layer with working memory systems
- Advanced pattern recognition using transformer architectures
- Multi-domain integration with cross-domain centrality analysis
- Adversarial throttle detection for cognitive bottlenecks
- Knowledge distillation from complex cognitive models
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Optional, Any, Tuple, Union
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from scipy.sparse import csr_matrix
from scipy.optimize import minimize
from sklearn.manifold import TSNE
from transformers import AutoTokenizer, AutoModel
import networkx as nx
from collections import defaultdict, deque
import asyncio
import logging
from enum import Enum
import json
import pickle

logger = logging.getLogger(__name__)


class CognitiveState(Enum):
    """Represents different cognitive states during task execution."""
    FLOW = "flow"
    ANALYSIS_PARALYSIS = "analysis_paralysis"
    TUNNEL_VISION = "tunnel_vision"
    COGNITIVE_OVERLOAD = "cognitive_overload"
    DEFAULT_BEHAVIOR = "default_behavior"
    OPTIMAL_ENGAGEMENT = "optimal_engagement"
    METACOGNITIVE_REFLECTION = "metacognitive_reflection"


class ThrottleType(Enum):
    """Types of cognitive throttling mechanisms."""
    ATTENTION_LIMITATION = "attention_limitation"
    WORKING_MEMORY_SATURATION = "working_memory_saturation"
    DECISION_PARALYSIS = "decision_paralysis"
    INFORMATION_OVERLOAD = "information_overload"
    EXECUTIVE_EXHAUSTION = "executive_exhaustion"


@dataclass
class CognitiveLoad:
    """Represents multi-dimensional cognitive load."""
    intrinsic_load: float  # Task complexity
    extraneous_load: float  # Irrelevant information processing
    germane_load: float  # Schema construction/automation
    temporal_pressure: float  # Time-based stress
    emotional_valence: float  # Emotional impact
    metacognitive_demand: float  # Self-monitoring requirements
    confidence_uncertainty: float  # Decision confidence gaps
    
    @property
    def total_load(self) -> float:
        """Calculate weighted total cognitive load."""
        weights = [0.25, 0.15, 0.20, 0.15, 0.10, 0.10, 0.05]
        loads = [self.intrinsic_load, self.extraneous_load, self.germane_load,
                self.temporal_pressure, self.emotional_valence, 
                self.metacognitive_demand, self.confidence_uncertainty]
        return sum(w * l for w, l in zip(weights, loads))


@dataclass
class WorkingMemoryState:
    """Represents working memory system state."""
    phonological_loop: Dict[str, Any] = field(default_factory=dict)
    visuospatial_sketchpad: Dict[str, Any] = field(default_factory=dict)
    central_executive: Dict[str, Any] = field(default_factory=dict)
    episodic_buffer: List[Dict[str, Any]] = field(default_factory=list)
    attention_allocation: Dict[str, float] = field(default_factory=dict)
    capacity_utilization: float = 0.0
    
    def update_capacity(self):
        """Update working memory capacity utilization."""
        total_items = (len(self.phonological_loop) + 
                      len(self.visuospatial_sketchpad) + 
                      len(self.central_executive) + 
                      len(self.episodic_buffer))
        self.capacity_utilization = min(total_items / 7.0, 1.0)  # Miller's 7Â±2


class CognitiveTransformer(nn.Module):
    """Transformer architecture for cognitive pattern recognition."""
    
    def __init__(self, input_dim: int = 512, hidden_dim: int = 768, 
                 num_heads: int = 12, num_layers: int = 6):
        super().__init__()
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        
        # Input projection
        self.input_projection = nn.Linear(input_dim, hidden_dim)
        
        # Positional encoding
        self.positional_encoding = nn.Parameter(
            torch.randn(1000, hidden_dim) * 0.1
        )
        
        # Transformer layers
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=hidden_dim,
            nhead=num_heads,
            dim_feedforward=hidden_dim * 4,
            dropout=0.1,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)
        
        # Output heads for different cognitive patterns
        self.pattern_classifier = nn.Linear(hidden_dim, 7)  # 7 cognitive states
        self.load_predictor = nn.Linear(hidden_dim, 7)  # 7 load dimensions
        self.confidence_estimator = nn.Linear(hidden_dim, 1)
        
    def forward(self, x: torch.Tensor, attention_mask: Optional[torch.Tensor] = None):
        """Forward pass through cognitive transformer."""
        batch_size, seq_len = x.shape[:2]
        
        # Project input
        x = self.input_projection(x)
        
        # Add positional encoding
        pos_enc = self.positional_encoding[:seq_len].unsqueeze(0)
        x = x + pos_enc
        
        # Transform
        if attention_mask is not None:
            attention_mask = attention_mask.bool()
        
        x = self.transformer(x, src_key_padding_mask=attention_mask)
        
        # Pool sequence (mean of non-masked tokens)
        if attention_mask is not None:
            mask_expanded = (~attention_mask).unsqueeze(-1).float()
            x_pooled = (x * mask_expanded).sum(1) / mask_expanded.sum(1)
        else:
            x_pooled = x.mean(1)
        
        # Output predictions
        patterns = F.softmax(self.pattern_classifier(x_pooled), dim=-1)
        loads = torch.sigmoid(self.load_predictor(x_pooled))
        confidence = torch.sigmoid(self.confidence_estimator(x_pooled))
        
        return {
            'cognitive_patterns': patterns,
            'cognitive_loads': loads,
            'confidence': confidence,
            'hidden_states': x,
            'pooled_representation': x_pooled
        }


class MetacognitiveOrchestrator:
    """
    Metacognitive orchestration layer managing cognitive processes.
    
    Inspired by Combine Harvester's metacognitive orchestration and
    Four-Sided Triangle's process monitoring capabilities.
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.working_memory = WorkingMemoryState()
        self.process_monitor = ProcessMonitor()
        self.cognitive_transformer = CognitiveTransformer()
        self.knowledge_graph = nx.DiGraph()
        self.session_history = deque(maxlen=1000)
        self.throttle_detector = AdversarialThrottleDetector()
        
        # Load pre-trained embeddings if available
        try:
            self.tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')
            self.embedding_model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')
        except:
            logger.warning("Could not load pre-trained embeddings, using random initialization")
            self.tokenizer = None
            self.embedding_model = None
    
    def encode_cognitive_state(self, state_data: Dict[str, Any]) -> torch.Tensor:
        """Encode cognitive state into tensor representation."""
        features = []
        
        # Temporal features
        if 'timestamp' in state_data:
            ts = state_data['timestamp']
            hour_sin = np.sin(2 * np.pi * ts.hour / 24)
            hour_cos = np.cos(2 * np.pi * ts.hour / 24)
            day_sin = np.sin(2 * np.pi * ts.weekday() / 7)
            day_cos = np.cos(2 * np.pi * ts.weekday() / 7)
            features.extend([hour_sin, hour_cos, day_sin, day_cos])
        else:
            features.extend([0.0, 0.0, 0.0, 0.0])
        
        # Behavioral features
        if 'behavior' in state_data:
            behavior = state_data['behavior']
            duration_norm = min(behavior.get('duration', 0) / 3600, 1.0)  # Normalize to hours
            complexity_map = {'low': 0.2, 'medium': 0.5, 'high': 0.8, 'creative': 0.9, 'analytical': 0.7}
            complexity = complexity_map.get(behavior.get('complexity', 'medium'), 0.5)
            features.extend([duration_norm, complexity])
        else:
            features.extend([0.0, 0.5])
        
        # Context features
        if 'context' in state_data:
            context = state_data['context']
            domain_features = self._encode_domain(context.get('domain', 'general'))
            features.extend(domain_features)
        else:
            features.extend([0.0] * 10)  # Default domain encoding
        
        # Working memory features
        wm_features = [
            self.working_memory.capacity_utilization,
            len(self.working_memory.phonological_loop) / 7.0,
            len(self.working_memory.visuospatial_sketchpad) / 7.0,
            len(self.working_memory.central_executive) / 7.0,
            len(self.working_memory.episodic_buffer) / 7.0
        ]
        features.extend(wm_features)
        
        # Pad to fixed size
        while len(features) < 512:
            features.append(0.0)
        
        return torch.tensor(features[:512], dtype=torch.float32)
    
    def _encode_domain(self, domain: str) -> List[float]:
        """Encode domain into feature vector."""
        domain_map = {
            'general': [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
            'work': [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
            'health': [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
            'finance': [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
            'education': [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0],
            'social': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],
            'creative': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],
            'productivity': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],
            'research': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0],
            'other': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]
        }
        return domain_map.get(domain, domain_map['other'])
    
    async def process_cognitive_event(self, event_data: Dict[str, Any]) -> Dict[str, Any]:
        """Process cognitive event through metacognitive orchestration."""
        
        # Update working memory
        self._update_working_memory(event_data)
        
        # Encode current state
        state_tensor = self.encode_cognitive_state(event_data)
        
        # Run cognitive transformer
        with torch.no_grad():
            predictions = self.cognitive_transformer(state_tensor.unsqueeze(0))
        
        # Detect throttling
        throttle_analysis = await self.throttle_detector.analyze_event(event_data)
        
        # Update knowledge graph
        self._update_knowledge_graph(event_data, predictions)
        
        # Generate intervention recommendations
        interventions = self._generate_interventions(predictions, throttle_analysis)
        
        # Store in session history
        session_record = {
            'timestamp': datetime.now(),
            'event_data': event_data,
            'predictions': self._tensor_to_dict(predictions),
            'throttle_analysis': throttle_analysis,
            'interventions': interventions
        }
        self.session_history.append(session_record)
        
        return {
            'cognitive_state': self._interpret_cognitive_state(predictions),
            'cognitive_load': self._interpret_cognitive_load(predictions),
            'throttle_detection': throttle_analysis,
            'interventions': interventions,
            'confidence': predictions['confidence'].item(),
            'working_memory_status': self._get_working_memory_status()
        }
    
    def _update_working_memory(self, event_data: Dict[str, Any]):
        """Update working memory with new event data."""
        # Update phonological loop (verbal/auditory information)
        if 'verbal_content' in event_data:
            self.working_memory.phonological_loop['current_content'] = event_data['verbal_content']
        
        # Update visuospatial sketchpad (visual/spatial information)
        if 'visual_content' in event_data:
            self.working_memory.visuospatial_sketchpad['current_visual'] = event_data['visual_content']
        
        # Update central executive (control processes)
        if 'task_type' in event_data:
            self.working_memory.central_executive['current_task'] = event_data['task_type']
        
        # Update episodic buffer (integrated episodes)
        episode = {
            'timestamp': datetime.now(),
            'event_summary': str(event_data),
            'context': event_data.get('context', {})
        }
        self.working_memory.episodic_buffer.append(episode)
        
        # Maintain buffer size
        if len(self.working_memory.episodic_buffer) > 7:
            self.working_memory.episodic_buffer.pop(0)
        
        # Update capacity
        self.working_memory.update_capacity()
    
    def _update_knowledge_graph(self, event_data: Dict[str, Any], predictions: Dict[str, torch.Tensor]):
        """Update knowledge graph with event relationships."""
        event_id = f"event_{len(self.knowledge_graph.nodes)}"
        
        # Add event node
        self.knowledge_graph.add_node(
            event_id,
            type="cognitive_event",
            timestamp=datetime.now(),
            data=event_data,
            cognitive_state=predictions['cognitive_patterns'].argmax().item(),
            load_level=predictions['cognitive_loads'].mean().item()
        )
        
        # Connect to recent events
        recent_events = [n for n in self.knowledge_graph.nodes 
                        if self.knowledge_graph.nodes[n].get('type') == 'cognitive_event'][-5:]
        
        for recent_event in recent_events:
            if recent_event != event_id:
                # Calculate similarity and add edge if significant
                similarity = self._calculate_event_similarity(event_data, 
                    self.knowledge_graph.nodes[recent_event]['data'])
                if similarity > 0.3:
                    self.knowledge_graph.add_edge(recent_event, event_id, weight=similarity)
    
    def _calculate_event_similarity(self, event1: Dict[str, Any], event2: Dict[str, Any]) -> float:
        """Calculate similarity between two cognitive events."""
        similarity = 0.0
        total_weight = 0.0
        
        # Domain similarity
        if event1.get('domain') == event2.get('domain'):
            similarity += 0.3
        total_weight += 0.3
        
        # Task type similarity
        if event1.get('task_type') == event2.get('task_type'):
            similarity += 0.2
        total_weight += 0.2
        
        # Temporal proximity
        if 'timestamp' in event1 and 'timestamp' in event2:
            time_diff = abs((event1['timestamp'] - event2['timestamp']).total_seconds())
            temporal_sim = max(0, 1 - time_diff / 3600)  # Decay over 1 hour
            similarity += temporal_sim * 0.2
        total_weight += 0.2
        
        # Context overlap
        context1 = set(str(event1.get('context', {})).split())
        context2 = set(str(event2.get('context', {})).split())
        if context1 and context2:
            overlap = len(context1.intersection(context2)) / len(context1.union(context2))
            similarity += overlap * 0.3
        total_weight += 0.3
        
        return similarity / total_weight if total_weight > 0 else 0.0
    
    def _interpret_cognitive_state(self, predictions: Dict[str, torch.Tensor]) -> str:
        """Interpret cognitive state from model predictions."""
        state_probs = predictions['cognitive_patterns'][0]
        state_names = [state.value for state in CognitiveState]
        max_idx = state_probs.argmax().item()
        return state_names[max_idx]
    
    def _interpret_cognitive_load(self, predictions: Dict[str, torch.Tensor]) -> Dict[str, float]:
        """Interpret cognitive load from model predictions."""
        load_values = predictions['cognitive_loads'][0]
        load_names = ['intrinsic', 'extraneous', 'germane', 'temporal', 
                     'emotional', 'metacognitive', 'uncertainty']
        
        return {name: value.item() for name, value in zip(load_names, load_values)}
    
    def _generate_interventions(self, predictions: Dict[str, torch.Tensor], 
                               throttle_analysis: Dict[str, Any]) -> List[str]:
        """Generate intervention recommendations based on analysis."""
        interventions = []
        
        # State-based interventions
        cognitive_state = self._interpret_cognitive_state(predictions)
        if cognitive_state == CognitiveState.ANALYSIS_PARALYSIS.value:
            interventions.extend([
                "Set a decision deadline to prevent endless analysis",
                "Use the 80/20 rule - decide with 80% of information",
                "Break complex decisions into smaller, manageable parts"
            ])
        elif cognitive_state == CognitiveState.COGNITIVE_OVERLOAD.value:
            interventions.extend([
                "Reduce information sources and focus on essentials",
                "Take a strategic break to reset cognitive capacity",
                "Use external memory aids (notes, diagrams) to offload working memory"
            ])
        elif cognitive_state == CognitiveState.TUNNEL_VISION.value:
            interventions.extend([
                "Deliberately seek alternative perspectives",
                "Schedule regular domain-switching breaks",
                "Consult with experts from different fields"
            ])
        
        # Load-based interventions
        loads = self._interpret_cognitive_load(predictions)
        if loads['intrinsic'] > 0.8:
            interventions.append("Consider chunking complex tasks into smaller steps")
        if loads['extraneous'] > 0.7:
            interventions.append("Eliminate distractions and irrelevant information")
        if loads['temporal'] > 0.8:
            interventions.append("Reassess time constraints and consider deadline adjustments")
        
        # Throttle-based interventions
        if throttle_analysis.get('throttle_detected'):
            throttle_type = throttle_analysis.get('throttle_type')
            if throttle_type == ThrottleType.ATTENTION_LIMITATION.value:
                interventions.append("Use attention restoration techniques (nature break, meditation)")
            elif throttle_type == ThrottleType.WORKING_MEMORY_SATURATION.value:
                interventions.append("Externalize working memory through notes or mind maps")
        
        # Working memory interventions
        if self.working_memory.capacity_utilization > 0.9:
            interventions.append("Clear working memory by completing or documenting current tasks")
        
        return interventions
    
    def _get_working_memory_status(self) -> Dict[str, Any]:
        """Get current working memory status."""
        return {
            'capacity_utilization': self.working_memory.capacity_utilization,
            'phonological_items': len(self.working_memory.phonological_loop),
            'visuospatial_items': len(self.working_memory.visuospatial_sketchpad),
            'executive_items': len(self.working_memory.central_executive),
            'episodic_items': len(self.working_memory.episodic_buffer),
            'attention_allocation': dict(self.working_memory.attention_allocation)
        }
    
    def _tensor_to_dict(self, tensor_dict: Dict[str, torch.Tensor]) -> Dict[str, Any]:
        """Convert tensor dictionary to serializable format."""
        result = {}
        for key, value in tensor_dict.items():
            if isinstance(value, torch.Tensor):
                result[key] = value.detach().cpu().numpy().tolist()
            else:
                result[key] = value
        return result
    
    def get_cross_domain_analysis(self) -> Dict[str, Any]:
        """Perform cross-domain centrality analysis on knowledge graph."""
        if len(self.knowledge_graph.nodes) < 3:
            return {'message': 'Insufficient data for cross-domain analysis'}
        
        # Calculate centrality measures
        centrality_measures = {
            'betweenness': nx.betweenness_centrality(self.knowledge_graph),
            'closeness': nx.closeness_centrality(self.knowledge_graph),
            'eigenvector': nx.eigenvector_centrality(self.knowledge_graph, max_iter=1000),
            'pagerank': nx.pagerank(self.knowledge_graph)
        }
        
        # Domain-specific analysis
        domain_nodes = defaultdict(list)
        for node_id in self.knowledge_graph.nodes:
            node_data = self.knowledge_graph.nodes[node_id]
            domain = node_data.get('data', {}).get('domain', 'unknown')
            domain_nodes[domain].append(node_id)
        
        # Cross-domain connections
        cross_domain_edges = []
        for edge in self.knowledge_graph.edges:
            node1_domain = self.knowledge_graph.nodes[edge[0]].get('data', {}).get('domain', 'unknown')
            node2_domain = self.knowledge_graph.nodes[edge[1]].get('data', {}).get('domain', 'unknown')
            if node1_domain != node2_domain:
                cross_domain_edges.append((edge, node1_domain, node2_domain))
        
        return {
            'centrality_measures': centrality_measures,
            'domain_distribution': {domain: len(nodes) for domain, nodes in domain_nodes.items()},
            'cross_domain_connections': len(cross_domain_edges),
            'total_nodes': len(self.knowledge_graph.nodes),
            'total_edges': len(self.knowledge_graph.edges),
            'graph_density': nx.density(self.knowledge_graph)
        }


class AdversarialThrottleDetector:
    """
    Detects cognitive throttling mechanisms that limit performance.
    
    Inspired by Four-Sided Triangle's Adversarial Throttle Detection and Bypass system.
    """
    
    def __init__(self):
        self.throttle_patterns = {
            ThrottleType.ATTENTION_LIMITATION: {
                'indicators': ['attention_wandering', 'focus_breaks', 'distraction_events'],
                'thresholds': {'attention_duration': 0.2, 'focus_score': 0.3}
            },
            ThrottleType.WORKING_MEMORY_SATURATION: {
                'indicators': ['memory_errors', 'information_loss', 'task_switching_cost'],
                'thresholds': {'memory_load': 0.9, 'error_rate': 0.1}
            },
            ThrottleType.DECISION_PARALYSIS: {
                'indicators': ['decision_delay', 'option_proliferation', 'analysis_loops'],
                'thresholds': {'decision_time': 3.0, 'option_count': 5}
            },
            ThrottleType.INFORMATION_OVERLOAD: {
                'indicators': ['information_volume', 'processing_delays', 'comprehension_gaps'],
                'thresholds': {'info_rate': 0.8, 'processing_lag': 2.0}
            },
            ThrottleType.EXECUTIVE_EXHAUSTION: {
                'indicators': ['control_failures', 'impulse_responses', 'planning_degradation'],
                'thresholds': {'control_score': 0.4, 'planning_quality': 0.5}
            }
        }
        
        self.detection_history = deque(maxlen=100)
    
    async def analyze_event(self, event_data: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze event for throttling patterns."""
        throttle_scores = {}
        detected_throttles = []
        
        for throttle_type, pattern in self.throttle_patterns.items():
            score = self._calculate_throttle_score(event_data, pattern)
            throttle_scores[throttle_type.value] = score
            
            if score > 0.7:  # Throttle detection threshold
                detected_throttles.append(throttle_type.value)
        
        # Pattern analysis across history
        pattern_analysis = self._analyze_throttle_patterns()
        
        result = {
            'throttle_detected': len(detected_throttles) > 0,
            'detected_throttles': detected_throttles,
            'throttle_scores': throttle_scores,
            'pattern_analysis': pattern_analysis,
            'bypass_strategies': self._generate_bypass_strategies(detected_throttles)
        }
        
        self.detection_history.append({
            'timestamp': datetime.now(),
            'analysis': result
        })
        
        return result
    
    def _calculate_throttle_score(self, event_data: Dict[str, Any], pattern: Dict[str, Any]) -> float:
        """Calculate throttle score for a specific pattern."""
        score = 0.0
        indicator_count = 0
        
        # Check pattern indicators
        for indicator in pattern['indicators']:
            if indicator in event_data:
                score += 1.0
                indicator_count += 1
        
        # Check threshold violations
        thresholds = pattern['thresholds']
        threshold_violations = 0
        
        for threshold_key, threshold_value in thresholds.items():
            if threshold_key in event_data:
                event_value = event_data[threshold_key]
                if isinstance(event_value, (int, float)):
                    if event_value > threshold_value:
                        threshold_violations += 1
        
        # Combine indicator presence and threshold violations
        indicator_score = score / len(pattern['indicators']) if pattern['indicators'] else 0
        threshold_score = threshold_violations / len(thresholds) if thresholds else 0
        
        return (indicator_score * 0.6 + threshold_score * 0.4)
    
    def _analyze_throttle_patterns(self) -> Dict[str, Any]:
        """Analyze patterns in throttle detection history."""
        if len(self.detection_history) < 5:
            return {'message': 'Insufficient history for pattern analysis'}
        
        # Frequency analysis
        throttle_frequencies = defaultdict(int)
        for record in self.detection_history:
            for throttle in record['analysis']['detected_throttles']:
                throttle_frequencies[throttle] += 1
        
        # Temporal patterns
        recent_throttles = [record['analysis']['detected_throttles'] 
                          for record in list(self.detection_history)[-10:]]
        
        # Escalation detection
        escalation_pattern = self._detect_escalation()
        
        return {
            'frequent_throttles': dict(throttle_frequencies),
            'recent_pattern': recent_throttles,
            'escalation_detected': escalation_pattern,
            'total_detections': sum(throttle_frequencies.values())
        }
    
    def _detect_escalation(self) -> bool:
        """Detect if throttling is escalating over time."""
        if len(self.detection_history) < 10:
            return False
        
        # Compare first half vs second half of recent history
        mid_point = len(self.detection_history) // 2
        first_half = list(self.detection_history)[:mid_point]
        second_half = list(self.detection_history)[mid_point:]
        
        first_avg = np.mean([len(record['analysis']['detected_throttles']) for record in first_half])
        second_avg = np.mean([len(record['analysis']['detected_throttles']) for record in second_half])
        
        return second_avg > first_avg * 1.5  # 50% increase indicates escalation
    
    def _generate_bypass_strategies(self, detected_throttles: List[str]) -> List[str]:
        """Generate strategies to bypass detected throttles."""
        strategies = []
        
        strategy_map = {
            ThrottleType.ATTENTION_LIMITATION.value: [
                "Implement attention restoration breaks every 25 minutes",
                "Use environmental cues to maintain focus",
                "Practice selective attention training exercises"
            ],
            ThrottleType.WORKING_MEMORY_SATURATION.value: [
                "Externalize working memory through structured note-taking",
                "Use chunking strategies to reduce memory load",
                "Implement progressive disclosure of information"
            ],
            ThrottleType.DECISION_PARALYSIS.value: [
                "Set decision criteria and scoring systems beforehand",
                "Use elimination strategies to reduce options",
                "Implement time-boxed decision making"
            ],
            ThrottleType.INFORMATION_OVERLOAD.value: [
                "Filter information sources by relevance and quality",
                "Use hierarchical information processing",
                "Implement information diet principles"
            ],
            ThrottleType.EXECUTIVE_EXHAUSTION.value: [
                "Schedule high-control tasks during peak energy periods",
                "Use automation and habit formation to reduce control demands",
                "Implement strategic rest and recovery protocols"
            ]
        }
        
        for throttle in detected_throttles:
            if throttle in strategy_map:
                strategies.extend(strategy_map[throttle])
        
        return list(set(strategies))  # Remove duplicates


class ProcessMonitor:
    """
    Monitors cognitive processes and provides quality assessment.
    
    Inspired by Four-Sided Triangle's Process Monitor component.
    """
    
    def __init__(self):
        self.quality_metrics = {
            'completeness': 0.0,
            'consistency': 0.0,
            'confidence': 0.0,
            'compliance': 0.0,
            'correctness': 0.0
        }
        self.process_history = deque(maxlen=200)
    
    def evaluate_process_quality(self, process_data: Dict[str, Any]) -> Dict[str, float]:
        """Evaluate quality across multiple dimensions."""
        
        # Completeness: Are all required elements present?
        completeness = self._assess_completeness(process_data)
        
        # Consistency: Are responses consistent with previous patterns?
        consistency = self._assess_consistency(process_data)
        
        # Confidence: How confident is the system in its analysis?
        confidence = process_data.get('confidence', 0.5)
        
        # Compliance: Does the process follow established protocols?
        compliance = self._assess_compliance(process_data)
        
        # Correctness: Based on known ground truth and validation
        correctness = self._assess_correctness(process_data)
        
        quality_scores = {
            'completeness': completeness,
            'consistency': consistency,
            'confidence': confidence,
            'compliance': compliance,
            'correctness': correctness
        }
        
        # Store in history
        self.process_history.append({
            'timestamp': datetime.now(),
            'quality_scores': quality_scores,
            'process_data': process_data
        })
        
        return quality_scores
    
    def _assess_completeness(self, process_data: Dict[str, Any]) -> float:
        """Assess completeness of cognitive process."""
        required_fields = ['cognitive_state', 'cognitive_load', 'interventions']
        present_fields = sum(1 for field in required_fields if field in process_data)
        return present_fields / len(required_fields)
    
    def _assess_consistency(self, process_data: Dict[str, Any]) -> float:
        """Assess consistency with historical patterns."""
        if len(self.process_history) < 3:
            return 0.8  # Default when insufficient history
        
        # Compare with recent patterns
        recent_states = [record['process_data'].get('cognitive_state') 
                        for record in list(self.process_history)[-5:]]
        
        current_state = process_data.get('cognitive_state')
        if current_state in recent_states:
            return 0.9  # High consistency
        else:
            return 0.6  # Lower consistency (not necessarily bad)
    
    def _assess_compliance(self, process_data: Dict[str, Any]) -> float:
        """Assess compliance with cognitive processing protocols."""
        compliance_score = 1.0
        
        # Check if interventions are provided when needed
        if process_data.get('cognitive_load', {}).get('total_load', 0) > 0.8:
            if not process_data.get('interventions'):
                compliance_score -= 0.3
        
        # Check if confidence is reported
        if 'confidence' not in process_data:
            compliance_score -= 0.2
        
        return max(compliance_score, 0.0)
    
    def _assess_correctness(self, process_data: Dict[str, Any]) -> float:
        """Assess correctness based on validation rules."""
        correctness_score = 1.0
        
        # Validate cognitive load ranges
        if 'cognitive_load' in process_data:
            for load_type, value in process_data['cognitive_load'].items():
                if not (0.0 <= value <= 1.0):
                    correctness_score -= 0.1
        
        # Validate confidence ranges
        confidence = process_data.get('confidence', 0.5)
        if not (0.0 <= confidence <= 1.0):
            correctness_score -= 0.2
        
        return max(correctness_score, 0.0)
    
    def get_quality_trends(self) -> Dict[str, Any]:
        """Analyze quality trends over time."""
        if len(self.process_history) < 10:
            return {'message': 'Insufficient data for trend analysis'}
        
        # Calculate moving averages
        recent_scores = [record['quality_scores'] for record in list(self.process_history)[-10:]]
        
        trends = {}
        for metric in self.quality_metrics.keys():
            values = [scores[metric] for scores in recent_scores]
            trends[metric] = {
                'current': values[-1],
                'average': np.mean(values),
                'trend': 'improving' if values[-1] > np.mean(values[:-1]) else 'declining',
                'volatility': np.std(values)
            }
        
        return trends 